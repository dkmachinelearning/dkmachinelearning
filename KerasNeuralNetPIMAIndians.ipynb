{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGh49oQkGoUoS6aSATjERx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkmachinelearning/dkmachinelearning/blob/main/KerasNeuralNetPIMAIndians.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nqCj135ry9g7"
      },
      "outputs": [],
      "source": [
        "# first neural network with keras tutorial\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "dataset = read_csv('/content/diabetes.csv', delimiter=',')\n",
        "# split into input (X) and output (y) variables\n",
        "X = dataset.drop('Outcome', axis = 1)\n",
        "y = dataset['Outcome']"
      ],
      "metadata": {
        "id": "CaHHLUx6zKYi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we need to do k-fold CV"
      ],
      "metadata": {
        "id": "zU67EEZS_slT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the keras model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X, y, epochs=150, batch_size=10, verbose= 0)\n",
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(X, y)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30ky8ATlzK8j",
        "outputId": "80901f8b-2d0c-49e1-a941-9fc8988ad05a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 1s 41ms/step - loss: 0.4801 - accuracy: 0.7812\n",
            "Accuracy: 78.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HyperOptim with activation function for first two layers\n",
        "activation = ['relu',\n",
        "'sigmoid',\n",
        "'softmax',\n",
        "'softplus',\n",
        "'softsign',\n",
        "'tanh',\n",
        "'selu',\n",
        "'elu',\n",
        "'exponential',\n",
        "'leaky_relu',\n",
        "'relu6',\n",
        "'silu',\n",
        "'gelu',\n",
        "'linear',\n",
        "'mish',\n",
        "'log_softmax']\n",
        "\n",
        "for el1 in activation:\n",
        "  for el2 in activation:\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_shape=(8,), activation = el1))\n",
        "    model.add(Dense(8, activation = el2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # compile the keras model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # fit the keras model on the dataset\n",
        "    model.fit(X, y, epochs=150, batch_size=10, verbose= 0)\n",
        "    # evaluate the keras model\n",
        "    _, accuracy = model.evaluate(X, y)\n",
        "    print(f'Accuracy: {round(accuracy*100, 2)}%. Act. function for layer 1: {el1}. Act. function for layer 2: {el2}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C43iVmfR30XN",
        "outputId": "7542e520-3cec-4e74-dab8-d5edc2427b8b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 1s 30ms/step - loss: 0.4540 - accuracy: 0.7904\n",
            "Accuracy: 79.04%. Act. function for layer 1: relu. Act. function for layer 2: relu.\n",
            "24/24 [==============================] - 1s 32ms/step - loss: 0.5473 - accuracy: 0.7344\n",
            "Accuracy: 73.44%. Act. function for layer 1: relu. Act. function for layer 2: sigmoid.\n",
            "24/24 [==============================] - 1s 25ms/step - loss: 0.5241 - accuracy: 0.7422\n",
            "Accuracy: 74.22%. Act. function for layer 1: relu. Act. function for layer 2: softmax.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.7734\n",
            "Accuracy: 77.34%. Act. function for layer 1: relu. Act. function for layer 2: softplus.\n",
            "24/24 [==============================] - 1s 28ms/step - loss: 0.5401 - accuracy: 0.7266\n",
            "Accuracy: 72.66%. Act. function for layer 1: relu. Act. function for layer 2: softsign.\n",
            "24/24 [==============================] - 0s 15ms/step - loss: 0.5294 - accuracy: 0.7279\n",
            "Accuracy: 72.79%. Act. function for layer 1: relu. Act. function for layer 2: tanh.\n",
            "24/24 [==============================] - 1s 42ms/step - loss: 0.4626 - accuracy: 0.7839\n",
            "Accuracy: 78.39%. Act. function for layer 1: relu. Act. function for layer 2: selu.\n",
            "24/24 [==============================] - 1s 15ms/step - loss: 0.4761 - accuracy: 0.7630\n",
            "Accuracy: 76.3%. Act. function for layer 1: relu. Act. function for layer 2: elu.\n",
            "24/24 [==============================] - 1s 36ms/step - loss: nan - accuracy: 0.6510\n",
            "Accuracy: 65.1%. Act. function for layer 1: relu. Act. function for layer 2: exponential.\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.5177 - accuracy: 0.7526\n",
            "Accuracy: 75.26%. Act. function for layer 1: relu. Act. function for layer 2: leaky_relu.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7305\n",
            "Accuracy: 73.05%. Act. function for layer 1: relu. Act. function for layer 2: relu6.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4995 - accuracy: 0.7513\n",
            "Accuracy: 75.13%. Act. function for layer 1: relu. Act. function for layer 2: silu.\n",
            "24/24 [==============================] - 1s 27ms/step - loss: 0.4857 - accuracy: 0.7826\n",
            "Accuracy: 78.26%. Act. function for layer 1: relu. Act. function for layer 2: gelu.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5436 - accuracy: 0.7383\n",
            "Accuracy: 73.83%. Act. function for layer 1: relu. Act. function for layer 2: linear.\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.4926 - accuracy: 0.7656\n",
            "Accuracy: 76.56%. Act. function for layer 1: relu. Act. function for layer 2: mish.\n",
            "24/24 [==============================] - 1s 32ms/step - loss: 0.5049 - accuracy: 0.7552\n",
            "Accuracy: 75.52%. Act. function for layer 1: relu. Act. function for layer 2: log_softmax.\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.5127 - accuracy: 0.7565\n",
            "Accuracy: 75.65%. Act. function for layer 1: sigmoid. Act. function for layer 2: relu.\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.5194 - accuracy: 0.7552\n",
            "Accuracy: 75.52%. Act. function for layer 1: sigmoid. Act. function for layer 2: sigmoid.\n",
            "24/24 [==============================] - 1s 35ms/step - loss: 0.5064 - accuracy: 0.7630\n",
            "Accuracy: 76.3%. Act. function for layer 1: sigmoid. Act. function for layer 2: softmax.\n",
            "24/24 [==============================] - 1s 17ms/step - loss: 0.5486 - accuracy: 0.7174\n",
            "Accuracy: 71.74%. Act. function for layer 1: sigmoid. Act. function for layer 2: softplus.\n",
            "24/24 [==============================] - 1s 26ms/step - loss: 0.5242 - accuracy: 0.7188\n",
            "Accuracy: 71.88%. Act. function for layer 1: sigmoid. Act. function for layer 2: softsign.\n",
            "24/24 [==============================] - 1s 26ms/step - loss: 0.5057 - accuracy: 0.7396\n",
            "Accuracy: 73.96%. Act. function for layer 1: sigmoid. Act. function for layer 2: tanh.\n",
            "24/24 [==============================] - 1s 20ms/step - loss: 0.5459 - accuracy: 0.7279\n",
            "Accuracy: 72.79%. Act. function for layer 1: sigmoid. Act. function for layer 2: selu.\n",
            "24/24 [==============================] - 0s 7ms/step - loss: 0.5169 - accuracy: 0.7396\n",
            "Accuracy: 73.96%. Act. function for layer 1: sigmoid. Act. function for layer 2: elu.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5448 - accuracy: 0.7148\n",
            "Accuracy: 71.48%. Act. function for layer 1: sigmoid. Act. function for layer 2: exponential.\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.5372 - accuracy: 0.7188\n",
            "Accuracy: 71.88%. Act. function for layer 1: sigmoid. Act. function for layer 2: leaky_relu.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.7669\n",
            "Accuracy: 76.69%. Act. function for layer 1: sigmoid. Act. function for layer 2: relu6.\n",
            "24/24 [==============================] - 1s 34ms/step - loss: 0.5298 - accuracy: 0.7174\n",
            "Accuracy: 71.74%. Act. function for layer 1: sigmoid. Act. function for layer 2: silu.\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.5178 - accuracy: 0.7344\n",
            "Accuracy: 73.44%. Act. function for layer 1: sigmoid. Act. function for layer 2: gelu.\n",
            "24/24 [==============================] - 1s 21ms/step - loss: 0.5283 - accuracy: 0.7253\n",
            "Accuracy: 72.53%. Act. function for layer 1: sigmoid. Act. function for layer 2: linear.\n",
            "24/24 [==============================] - 1s 34ms/step - loss: 0.5090 - accuracy: 0.7513\n",
            "Accuracy: 75.13%. Act. function for layer 1: sigmoid. Act. function for layer 2: mish.\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.5354 - accuracy: 0.7305\n",
            "Accuracy: 73.05%. Act. function for layer 1: sigmoid. Act. function for layer 2: log_softmax.\n",
            "24/24 [==============================] - 0s 6ms/step - loss: 0.5492 - accuracy: 0.7201\n",
            "Accuracy: 72.01%. Act. function for layer 1: softmax. Act. function for layer 2: relu.\n",
            "24/24 [==============================] - 0s 7ms/step - loss: 0.5589 - accuracy: 0.6940\n",
            "Accuracy: 69.4%. Act. function for layer 1: softmax. Act. function for layer 2: sigmoid.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.6058 - accuracy: 0.7018\n",
            "Accuracy: 70.18%. Act. function for layer 1: softmax. Act. function for layer 2: softmax.\n",
            "24/24 [==============================] - 1s 18ms/step - loss: 0.5830 - accuracy: 0.6719\n",
            "Accuracy: 67.19%. Act. function for layer 1: softmax. Act. function for layer 2: softplus.\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.5512 - accuracy: 0.7279\n",
            "Accuracy: 72.79%. Act. function for layer 1: softmax. Act. function for layer 2: softsign.\n",
            "24/24 [==============================] - 1s 25ms/step - loss: 0.5880 - accuracy: 0.6602\n",
            "Accuracy: 66.02%. Act. function for layer 1: softmax. Act. function for layer 2: tanh.\n",
            "24/24 [==============================] - 1s 23ms/step - loss: 0.5486 - accuracy: 0.7201\n",
            "Accuracy: 72.01%. Act. function for layer 1: softmax. Act. function for layer 2: selu.\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.5406 - accuracy: 0.7357\n",
            "Accuracy: 73.57%. Act. function for layer 1: softmax. Act. function for layer 2: elu.\n",
            "24/24 [==============================] - 1s 34ms/step - loss: 0.6469 - accuracy: 0.6510\n",
            "Accuracy: 65.1%. Act. function for layer 1: softmax. Act. function for layer 2: exponential.\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.5811 - accuracy: 0.7096\n",
            "Accuracy: 70.96%. Act. function for layer 1: softmax. Act. function for layer 2: leaky_relu.\n",
            "24/24 [==============================] - 1s 35ms/step - loss: 0.5423 - accuracy: 0.7448\n",
            "Accuracy: 74.48%. Act. function for layer 1: softmax. Act. function for layer 2: relu6.\n",
            "24/24 [==============================] - 1s 17ms/step - loss: 0.5473 - accuracy: 0.7057\n",
            "Accuracy: 70.57%. Act. function for layer 1: softmax. Act. function for layer 2: silu.\n",
            "24/24 [==============================] - 1s 26ms/step - loss: 0.6048 - accuracy: 0.6536\n",
            "Accuracy: 65.36%. Act. function for layer 1: softmax. Act. function for layer 2: gelu.\n",
            "24/24 [==============================] - 1s 18ms/step - loss: 0.5465 - accuracy: 0.7266\n",
            "Accuracy: 72.66%. Act. function for layer 1: softmax. Act. function for layer 2: linear.\n",
            "24/24 [==============================] - 1s 20ms/step - loss: 0.5461 - accuracy: 0.6862\n",
            "Accuracy: 68.62%. Act. function for layer 1: softmax. Act. function for layer 2: mish.\n",
            "24/24 [==============================] - 1s 18ms/step - loss: 0.5617 - accuracy: 0.6810\n",
            "Accuracy: 68.1%. Act. function for layer 1: softmax. Act. function for layer 2: log_softmax.\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.4759 - accuracy: 0.7630\n",
            "Accuracy: 76.3%. Act. function for layer 1: softplus. Act. function for layer 2: relu.\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.5448 - accuracy: 0.6849\n",
            "Accuracy: 68.49%. Act. function for layer 1: softplus. Act. function for layer 2: sigmoid.\n",
            "24/24 [==============================] - 1s 19ms/step - loss: 0.6383 - accuracy: 0.6615\n",
            "Accuracy: 66.15%. Act. function for layer 1: softplus. Act. function for layer 2: softmax.\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.4998 - accuracy: 0.7526\n",
            "Accuracy: 75.26%. Act. function for layer 1: softplus. Act. function for layer 2: softplus.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4781 - accuracy: 0.7578\n",
            "Accuracy: 75.78%. Act. function for layer 1: softplus. Act. function for layer 2: softsign.\n",
            "24/24 [==============================] - 1s 27ms/step - loss: 0.5452 - accuracy: 0.7383\n",
            "Accuracy: 73.83%. Act. function for layer 1: softplus. Act. function for layer 2: tanh.\n",
            "24/24 [==============================] - 1s 28ms/step - loss: 0.4427 - accuracy: 0.7852\n",
            "Accuracy: 78.52%. Act. function for layer 1: softplus. Act. function for layer 2: selu.\n",
            "24/24 [==============================] - 1s 34ms/step - loss: 0.4533 - accuracy: 0.7812\n",
            "Accuracy: 78.12%. Act. function for layer 1: softplus. Act. function for layer 2: elu.\n",
            "24/24 [==============================] - 1s 16ms/step - loss: nan - accuracy: 0.6510\n",
            "Accuracy: 65.1%. Act. function for layer 1: softplus. Act. function for layer 2: exponential.\n",
            "24/24 [==============================] - 1s 23ms/step - loss: 0.4680 - accuracy: 0.7734\n",
            "Accuracy: 77.34%. Act. function for layer 1: softplus. Act. function for layer 2: leaky_relu.\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.5237 - accuracy: 0.7266\n",
            "Accuracy: 72.66%. Act. function for layer 1: softplus. Act. function for layer 2: relu6.\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.4710 - accuracy: 0.7865\n",
            "Accuracy: 78.65%. Act. function for layer 1: softplus. Act. function for layer 2: silu.\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.4826 - accuracy: 0.7682\n",
            "Accuracy: 76.82%. Act. function for layer 1: softplus. Act. function for layer 2: gelu.\n",
            "24/24 [==============================] - 1s 17ms/step - loss: 0.4659 - accuracy: 0.7773\n",
            "Accuracy: 77.73%. Act. function for layer 1: softplus. Act. function for layer 2: linear.\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.4814 - accuracy: 0.7786\n",
            "Accuracy: 77.86%. Act. function for layer 1: softplus. Act. function for layer 2: mish.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4476 - accuracy: 0.7969\n",
            "Accuracy: 79.69%. Act. function for layer 1: softplus. Act. function for layer 2: log_softmax.\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.4817 - accuracy: 0.7630\n",
            "Accuracy: 76.3%. Act. function for layer 1: softsign. Act. function for layer 2: relu.\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.5164 - accuracy: 0.7578\n",
            "Accuracy: 75.78%. Act. function for layer 1: softsign. Act. function for layer 2: sigmoid.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.7487\n",
            "Accuracy: 74.87%. Act. function for layer 1: softsign. Act. function for layer 2: softmax.\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.5092 - accuracy: 0.7435\n",
            "Accuracy: 74.35%. Act. function for layer 1: softsign. Act. function for layer 2: softplus.\n",
            "24/24 [==============================] - 1s 23ms/step - loss: 0.5170 - accuracy: 0.7331\n",
            "Accuracy: 73.31%. Act. function for layer 1: softsign. Act. function for layer 2: softsign.\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.5088 - accuracy: 0.7513\n",
            "Accuracy: 75.13%. Act. function for layer 1: softsign. Act. function for layer 2: tanh.\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.4939 - accuracy: 0.7591\n",
            "Accuracy: 75.91%. Act. function for layer 1: softsign. Act. function for layer 2: selu.\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4954 - accuracy: 0.7591\n",
            "Accuracy: 75.91%. Act. function for layer 1: softsign. Act. function for layer 2: elu.\n",
            "24/24 [==============================] - 0s 7ms/step - loss: 0.5407 - accuracy: 0.7201\n",
            "Accuracy: 72.01%. Act. function for layer 1: softsign. Act. function for layer 2: exponential.\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.5417 - accuracy: 0.7096\n",
            "Accuracy: 70.96%. Act. function for layer 1: softsign. Act. function for layer 2: leaky_relu.\n",
            "24/24 [==============================] - 1s 25ms/step - loss: 0.5146 - accuracy: 0.7409\n",
            "Accuracy: 74.09%. Act. function for layer 1: softsign. Act. function for layer 2: relu6.\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.5311 - accuracy: 0.7344\n",
            "Accuracy: 73.44%. Act. function for layer 1: softsign. Act. function for layer 2: silu.\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.5090 - accuracy: 0.7539\n",
            "Accuracy: 75.39%. Act. function for layer 1: softsign. Act. function for layer 2: gelu.\n",
            "24/24 [==============================] - 1s 34ms/step - loss: 0.5325 - accuracy: 0.7461\n",
            "Accuracy: 74.61%. Act. function for layer 1: softsign. Act. function for layer 2: linear.\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.5262 - accuracy: 0.7435\n",
            "Accuracy: 74.35%. Act. function for layer 1: softsign. Act. function for layer 2: mish.\n",
            "24/24 [==============================] - 1s 35ms/step - loss: 0.5157 - accuracy: 0.7253\n",
            "Accuracy: 72.53%. Act. function for layer 1: softsign. Act. function for layer 2: log_softmax.\n",
            "24/24 [==============================] - 1s 19ms/step - loss: 0.5764 - accuracy: 0.6693\n",
            "Accuracy: 66.93%. Act. function for layer 1: tanh. Act. function for layer 2: relu.\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.5479 - accuracy: 0.7070\n",
            "Accuracy: 70.7%. Act. function for layer 1: tanh. Act. function for layer 2: sigmoid.\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.5371 - accuracy: 0.7253\n",
            "Accuracy: 72.53%. Act. function for layer 1: tanh. Act. function for layer 2: softmax.\n",
            "24/24 [==============================] - 1s 25ms/step - loss: 0.5631 - accuracy: 0.6966\n",
            "Accuracy: 69.66%. Act. function for layer 1: tanh. Act. function for layer 2: softplus.\n",
            "24/24 [==============================] - 1s 34ms/step - loss: 0.5702 - accuracy: 0.6875\n",
            "Accuracy: 68.75%. Act. function for layer 1: tanh. Act. function for layer 2: softsign.\n",
            "24/24 [==============================] - 0s 15ms/step - loss: 0.5433 - accuracy: 0.7096\n",
            "Accuracy: 70.96%. Act. function for layer 1: tanh. Act. function for layer 2: tanh.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5437 - accuracy: 0.7279\n",
            "Accuracy: 72.79%. Act. function for layer 1: tanh. Act. function for layer 2: selu.\n",
            "24/24 [==============================] - 1s 22ms/step - loss: 0.5028 - accuracy: 0.7435\n",
            "Accuracy: 74.35%. Act. function for layer 1: tanh. Act. function for layer 2: elu.\n",
            "24/24 [==============================] - 1s 27ms/step - loss: 0.5178 - accuracy: 0.7292\n",
            "Accuracy: 72.92%. Act. function for layer 1: tanh. Act. function for layer 2: exponential.\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.5817 - accuracy: 0.6654\n",
            "Accuracy: 66.54%. Act. function for layer 1: tanh. Act. function for layer 2: leaky_relu.\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5566 - accuracy: 0.6771\n",
            "Accuracy: 67.71%. Act. function for layer 1: tanh. Act. function for layer 2: relu6.\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.5772 - accuracy: 0.6914\n",
            "Accuracy: 69.14%. Act. function for layer 1: tanh. Act. function for layer 2: silu.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5697 - accuracy: 0.6836\n",
            "Accuracy: 68.36%. Act. function for layer 1: tanh. Act. function for layer 2: gelu.\n",
            "24/24 [==============================] - 1s 18ms/step - loss: 0.5624 - accuracy: 0.6888\n",
            "Accuracy: 68.88%. Act. function for layer 1: tanh. Act. function for layer 2: linear.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.7057\n",
            "Accuracy: 70.57%. Act. function for layer 1: tanh. Act. function for layer 2: mish.\n",
            "24/24 [==============================] - 0s 15ms/step - loss: 0.5419 - accuracy: 0.7227\n",
            "Accuracy: 72.27%. Act. function for layer 1: tanh. Act. function for layer 2: log_softmax.\n",
            "24/24 [==============================] - 1s 19ms/step - loss: 0.4368 - accuracy: 0.7799\n",
            "Accuracy: 77.99%. Act. function for layer 1: selu. Act. function for layer 2: relu.\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.4485 - accuracy: 0.7943\n",
            "Accuracy: 79.43%. Act. function for layer 1: selu. Act. function for layer 2: sigmoid.\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.4757 - accuracy: 0.7565\n",
            "Accuracy: 75.65%. Act. function for layer 1: selu. Act. function for layer 2: softmax.\n",
            "24/24 [==============================] - 0s 15ms/step - loss: 0.4615 - accuracy: 0.7734\n",
            "Accuracy: 77.34%. Act. function for layer 1: selu. Act. function for layer 2: softplus.\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4800 - accuracy: 0.7826\n",
            "Accuracy: 78.26%. Act. function for layer 1: selu. Act. function for layer 2: softsign.\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.5508 - accuracy: 0.6810\n",
            "Accuracy: 68.1%. Act. function for layer 1: selu. Act. function for layer 2: tanh.\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.4232 - accuracy: 0.7917\n",
            "Accuracy: 79.17%. Act. function for layer 1: selu. Act. function for layer 2: selu.\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.4537 - accuracy: 0.7773\n",
            "Accuracy: 77.73%. Act. function for layer 1: selu. Act. function for layer 2: elu.\n",
            "24/24 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6510\n",
            "Accuracy: 65.1%. Act. function for layer 1: selu. Act. function for layer 2: exponential.\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.4399 - accuracy: 0.7734\n",
            "Accuracy: 77.34%. Act. function for layer 1: selu. Act. function for layer 2: leaky_relu.\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4356 - accuracy: 0.7852\n",
            "Accuracy: 78.52%. Act. function for layer 1: selu. Act. function for layer 2: relu6.\n",
            "24/24 [==============================] - 0s 6ms/step - loss: 0.4392 - accuracy: 0.7917\n",
            "Accuracy: 79.17%. Act. function for layer 1: selu. Act. function for layer 2: silu.\n",
            "24/24 [==============================] - 1s 19ms/step - loss: 0.5567 - accuracy: 0.7448\n",
            "Accuracy: 74.48%. Act. function for layer 1: selu. Act. function for layer 2: gelu.\n",
            "24/24 [==============================] - 1s 29ms/step - loss: 0.4686 - accuracy: 0.7904\n",
            "Accuracy: 79.04%. Act. function for layer 1: selu. Act. function for layer 2: linear.\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.4411 - accuracy: 0.7826\n",
            "Accuracy: 78.26%. Act. function for layer 1: selu. Act. function for layer 2: mish.\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.4623 - accuracy: 0.7839\n",
            "Accuracy: 78.39%. Act. function for layer 1: selu. Act. function for layer 2: log_softmax.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-aaaddcfda4e2>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# fit the keras model on the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m# evaluate the keras model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}