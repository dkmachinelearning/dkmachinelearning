{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9Vvs0HcWtnCF8nXpQhgp3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkmachinelearning/dkmachinelearning/blob/main/MLPRegressor_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FhaN24naD-oE"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "#\n",
        "# Load California housing data set\n",
        "#\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n"
      ],
      "metadata": {
        "id": "rgbi5vnhEN0O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "print(pd.DataFrame(X).describe())\n",
        "print('\\n')\n",
        "print(pd.DataFrame(X).info())"
      ],
      "metadata": {
        "id": "j7lVPMS0GCv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa15dab-e80f-4684-f80b-7aa92f3d3f34"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  0             1             2             3             4  \\\n",
            "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
            "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
            "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
            "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
            "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
            "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
            "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
            "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
            "\n",
            "                  5             6             7  \n",
            "count  20640.000000  20640.000000  20640.000000  \n",
            "mean       3.070655     35.631861   -119.569704  \n",
            "std       10.386050      2.135952      2.003532  \n",
            "min        0.692308     32.540000   -124.350000  \n",
            "25%        2.429741     33.930000   -121.800000  \n",
            "50%        2.818116     34.260000   -118.490000  \n",
            "75%        3.282261     37.710000   -118.010000  \n",
            "max     1243.333333     41.950000   -114.310000  \n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20640 entries, 0 to 20639\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       20640 non-null  float64\n",
            " 1   1       20640 non-null  float64\n",
            " 2   2       20640 non-null  float64\n",
            " 3   3       20640 non-null  float64\n",
            " 4   4       20640 non-null  float64\n",
            " 5   5       20640 non-null  float64\n",
            " 6   6       20640 non-null  float64\n",
            " 7   7       20640 non-null  float64\n",
            "dtypes: float64(8)\n",
            "memory usage: 1.3 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking NaNs - it's not nessesary\n",
        "nan_cols = []\n",
        "for col in pd.DataFrame(X).columns:\n",
        "    if pd.DataFrame(X)[col].isnull().any():\n",
        "        print(col, pd.DataFrame(X)[col].isnull().sum())\n",
        "        nan_cols.append(col)\n",
        "    else:\n",
        "      print('There\\'s no NaNs in data')"
      ],
      "metadata": {
        "id": "VyQx6ozCHYEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da33b145-4663-4096-c8d0-26f446204689"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There's no NaNs in data\n",
            "There's no NaNs in data\n",
            "There's no NaNs in data\n",
            "There's no NaNs in data\n",
            "There's no NaNs in data\n",
            "There's no NaNs in data\n",
            "There's no NaNs in data\n",
            "There's no NaNs in data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NN without scaling\")\n",
        "\n",
        "# Create training/ test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor\n",
        "nn = MLPRegressor(random_state=1)\n",
        "\n",
        "# Train the model\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "pred = nn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = nn.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "id": "w9b63zSAENt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d5c208-3d3e-427f-8106-7b8879b9b451"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN without scaling\n",
            "R_squared value: , 0.50 %\n",
            "RMSE: , 0.81 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NN with StandardScaler\")\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_ss = scaler.fit_transform(X)\n",
        "\n",
        "# Create training/ test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ss, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor\n",
        "nn = MLPRegressor(random_state=1)\n",
        "\n",
        "# Train the model\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "pred = nn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = nn.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "id": "EI1R1v-0ENnX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5c0103-9bd5-4b83-b129-3d264bfd107e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN with StandardScaler\n",
            "R_squared value: , 0.78 %\n",
            "RMSE: , 0.54 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NN with MinMaxScaler\")\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_mm = scaler.fit_transform(X)\n",
        "\n",
        "# Create training/ test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_mm, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor, more iterations needed\n",
        "nn = MLPRegressor(random_state=1, max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "pred = nn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = nn.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "id": "v5EmLdGGENhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95b7b28-f8fb-4705-84f4-ddcdcf7ec5df"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN with MinMaxScaler\n",
            "R_squared value: , 0.74 %\n",
            "RMSE: , 0.59 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NN with MaxAbsScaler\")\n",
        "\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_ma = scaler.fit_transform(X)\n",
        "\n",
        "# Create training/ test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ma, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor, more iterations needed\n",
        "nn = MLPRegressor(random_state=1, max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "pred = nn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = nn.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "id": "CzyYpY8gENah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e834bf3-afc3-430d-8b09-b5d85577889a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN with MaxAbsScaler\n",
            "R_squared value: , 0.74 %\n",
            "RMSE: , 0.59 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NN with RobustScaler\")\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_rb = scaler.fit_transform(X)\n",
        "\n",
        "# Create training/ test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_rb, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor, more iterations needed\n",
        "nn = MLPRegressor(random_state=1, max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "pred = nn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = nn.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "id": "3PPNGHmMENT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baeb988b-c1dd-4f15-bd74-9ed1feecda8d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN with RobustScaler\n",
            "R_squared value: , 0.74 %\n",
            "RMSE: , 0.59 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"StandardSlaer was the best. Now it's time for hyperparameters optimization\")\n",
        "\n",
        "#Last T&T split\n",
        "# Create training/ test data split with StandardScaler\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ss, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor, more max_iterations just in case\n",
        "nn = MLPRegressor(random_state=1, max_iter=5000)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#Found great trick for making combination of neurons in layers :)\n",
        "from itertools import product\n",
        "first_layer_neurons = np.arange(5, 20, 5)\n",
        "second_layer_neurons = np.arange(1, 20, 5)\n",
        "hidden_layer_sizes = list(product(first_layer_neurons, second_layer_neurons))\n",
        "\n",
        "#preparing dict with hyperparams for optimization\n",
        "param_list = {\n",
        "    \"hidden_layer_sizes\": hidden_layer_sizes,\n",
        "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
        "    \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
        "    \"alpha\": [0.001]}\n",
        "search = GridSearchCV(estimator=nn, param_grid=param_list, n_jobs = -1, cv = 3, verbose = 1)\n",
        "\n",
        "# Train the model\n",
        "search.fit(X_train, y_train)\n",
        "print(search.best_estimator_, search.best_params_)\n",
        "# Make prediction\n",
        "pred = search.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = search.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "id": "Af4pvgp0LB1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d9720ac-d36e-4107-de0b-47183c51a936"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StandardSlaer was the best. Now it's time for hyperparameters optimization\n",
            "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
            "MLPRegressor(activation='tanh', alpha=0.001, hidden_layer_sizes=(5, 16),\n",
            "             max_iter=5000, random_state=1, solver='lbfgs') {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 16), 'solver': 'lbfgs'}\n",
            "R_squared value: , 0.80 %\n",
            "RMSE: , 0.52 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ]
    }
  ]
}