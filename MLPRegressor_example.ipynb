{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUfJgKjTUu+kTaJUEamEbf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkmachinelearning/dkmachinelearning/blob/main/MLPRegressor_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FhaN24naD-oE"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "#\n",
        "# Load California housing data set\n",
        "#\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n"
      ],
      "metadata": {
        "id": "rgbi5vnhEN0O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "print(pd.DataFrame(X).describe())\n",
        "print('\\n')\n",
        "print(pd.DataFrame(X).info())"
      ],
      "metadata": {
        "id": "j7lVPMS0GCv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa15dab-e80f-4684-f80b-7aa92f3d3f34"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  0             1             2             3             4  \\\n",
            "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
            "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
            "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
            "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
            "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
            "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
            "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
            "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
            "\n",
            "                  5             6             7  \n",
            "count  20640.000000  20640.000000  20640.000000  \n",
            "mean       3.070655     35.631861   -119.569704  \n",
            "std       10.386050      2.135952      2.003532  \n",
            "min        0.692308     32.540000   -124.350000  \n",
            "25%        2.429741     33.930000   -121.800000  \n",
            "50%        2.818116     34.260000   -118.490000  \n",
            "75%        3.282261     37.710000   -118.010000  \n",
            "max     1243.333333     41.950000   -114.310000  \n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20640 entries, 0 to 20639\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       20640 non-null  float64\n",
            " 1   1       20640 non-null  float64\n",
            " 2   2       20640 non-null  float64\n",
            " 3   3       20640 non-null  float64\n",
            " 4   4       20640 non-null  float64\n",
            " 5   5       20640 non-null  float64\n",
            " 6   6       20640 non-null  float64\n",
            " 7   7       20640 non-null  float64\n",
            "dtypes: float64(8)\n",
            "memory usage: 1.3 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking NaNs - it's not nessesary\n",
        "nan_cols = []\n",
        "for col in pd.DataFrame(X).columns:\n",
        "    if pd.DataFrame(X)[col].isnull().any():\n",
        "        print(col, pd.DataFrame(X)[col].isnull().sum())\n",
        "        nan_cols.append(col)\n",
        "    else:\n",
        "      print('There\\'s no NaNs in data')"
      ],
      "metadata": {
        "id": "VyQx6ozCHYEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da33b145-4663-4096-c8d0-26f446204689"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There's no NaNs in data\n",
            "There's no NaNs in data\n",
            "There's no NaNs in data\n",
            "There's no NaNs in data\n",
            "There's no NaNs in data\n",
            "There's no NaNs in data\n",
            "There's no NaNs in data\n",
            "There's no NaNs in data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NN without scaling\")\n",
        "\n",
        "# Create training/ test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor\n",
        "nn = MLPRegressor(random_state=1)\n",
        "\n",
        "# Train the model\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "pred = nn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = nn.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "id": "w9b63zSAENt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d5c208-3d3e-427f-8106-7b8879b9b451"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN without scaling\n",
            "R_squared value: , 0.50 %\n",
            "RMSE: , 0.81 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NN with StandardScaler\")\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_ss = scaler.fit_transform(X)\n",
        "\n",
        "# Create training/ test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ss, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor\n",
        "nn = MLPRegressor(random_state=1)\n",
        "\n",
        "# Train the model\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "pred = nn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = nn.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "id": "EI1R1v-0ENnX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5c0103-9bd5-4b83-b129-3d264bfd107e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN with StandardScaler\n",
            "R_squared value: , 0.78 %\n",
            "RMSE: , 0.54 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NN with MinMaxScaler\")\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_mm = scaler.fit_transform(X)\n",
        "\n",
        "# Create training/ test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_mm, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor, more iterations needed\n",
        "nn = MLPRegressor(random_state=1, max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "pred = nn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = nn.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "id": "v5EmLdGGENhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95b7b28-f8fb-4705-84f4-ddcdcf7ec5df"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN with MinMaxScaler\n",
            "R_squared value: , 0.74 %\n",
            "RMSE: , 0.59 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NN with MaxAbsScaler\")\n",
        "\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_ma = scaler.fit_transform(X)\n",
        "\n",
        "# Create training/ test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ma, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor, more iterations needed\n",
        "nn = MLPRegressor(random_state=1, max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "pred = nn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = nn.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "id": "CzyYpY8gENah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e834bf3-afc3-430d-8b09-b5d85577889a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN with MaxAbsScaler\n",
            "R_squared value: , 0.74 %\n",
            "RMSE: , 0.59 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NN with RobustScaler\")\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_rb = scaler.fit_transform(X)\n",
        "\n",
        "# Create training/ test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_rb, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor, more iterations needed\n",
        "nn = MLPRegressor(random_state=1, max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "pred = nn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = nn.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "id": "3PPNGHmMENT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baeb988b-c1dd-4f15-bd74-9ed1feecda8d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN with RobustScaler\n",
            "R_squared value: , 0.74 %\n",
            "RMSE: , 0.59 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"StandardSlaer was the best. Now it's time for hyperparameters optimization\")\n",
        "\n",
        "#Last T&T split\n",
        "# Create training/ test data split with StandardScaler\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ss, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor, more max_iterations just in case\n",
        "nn = MLPRegressor(random_state=1, max_iter=200)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#Found great trick for making combination of neurons in layers :)\n",
        "from itertools import product\n",
        "first_layer_neurons = np.arange(5, 20, 5)\n",
        "second_layer_neurons = np.arange(0, 20, 5)\n",
        "hidden_layer_sizes = list(product(first_layer_neurons, second_layer_neurons))\n",
        "\n",
        "#preparing dict with hyperparams for optimization\n",
        "param_list = {\n",
        "    \"hidden_layer_sizes\": hidden_layer_sizes,\n",
        "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
        "    \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
        "    \"alpha\": np.arange(0.0005, 0.1, 0.05)}\n",
        "search = GridSearchCV(estimator=nn, param_grid=param_list, n_jobs = -1, cv = 3, verbose = 7)\n",
        "\n",
        "# Train the model\n",
        "search.fit(X_train, y_train)\n",
        "print(search.best_estimator_, search.best_params_)\n",
        "# Make prediction\n",
        "pred = search.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = search.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "id": "Af4pvgp0LB1K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "82eefc08-2fe3-4e7a-9574-a9209c7c63c9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StandardSlaer was the best. Now it's time for hyperparameters optimization\n",
            "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3507e1e4921c>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Make prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GridSearch last too long, let's try TuneSearchCV\n",
        "!pip install tune-sklearn \"ray[tune]\"\n",
        "#from tune_sklearn import TuneSearchCV"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTidOGps-t-q",
        "outputId": "ab209561-11e2-4dbc-d516-3db3f182c5fb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tune-sklearn in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: ray[tune] in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from tune-sklearn) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tune-sklearn) (1.11.4)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from tune-sklearn) (1.25.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.14.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (24.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.0.3)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (14.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2023.6.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.35.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->tune-sklearn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->tune-sklearn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ray[tune]) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tune_sklearn import TuneGridSearchCV\n",
        "\n",
        "print(\"Hyperparameters optimization with TuneSearchCV\")\n",
        "\n",
        "#Last T&T split\n",
        "# Create training/ test data split with StandardScaler\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ss, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Instantiate MLPRegressor, more max_iterations just in case\n",
        "nn = MLPRegressor(random_state=1, max_iter=200)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#Found great trick for making combination of neurons in layers :)\n",
        "from itertools import product\n",
        "first_layer_neurons = np.arange(5, 20, 5)\n",
        "second_layer_neurons = np.arange(0, 20, 5)\n",
        "hidden_layer_sizes = list(product(first_layer_neurons, second_layer_neurons))\n",
        "\n",
        "#preparing dict with hyperparams for optimization\n",
        "param_list = {\n",
        "    \"hidden_layer_sizes\": hidden_layer_sizes,\n",
        "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
        "    \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
        "    \"alpha\": np.arange(0.0005, 0.1, 0.05)}\n",
        "search = TuneSearchCV(estimator=nn, param_grid=param_list, n_jobs = -1, cv = 3, verbose = 7)\n",
        "\n",
        "# Train the model\n",
        "search.fit(X_train, y_train)\n",
        "print(search.best_estimator_, search.best_params_)\n",
        "# Make prediction\n",
        "pred = search.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and error metrics\n",
        "test_set_rsquared = search.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# Print R_squared and RMSE value\n",
        "print(f'R_squared value: , {test_set_rsquared:.2f} %')\n",
        "print(f'RMSE: , {test_set_rmse:.2f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "lwqABXal_bXb",
        "outputId": "8dd1b962-5858-4ddd-f967-bb33d85bb3ff"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ray.tune.search.skopt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-fc649686e1c9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtune_sklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuneGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hyperparameters optimization with TuneSearchCV\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Last T&T split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tune_sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtune_sklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune_gridsearch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuneGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtune_sklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuneSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtune_sklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"TuneGridSearchCV\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TuneSearchCV\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__version__\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tune_sklearn/tune_search.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedulers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHyperBandForBOHB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCombinedStopper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSkOptSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHyperOptSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptuna\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptunaSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray.tune.search.skopt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade ray\n",
        "#Not sure why ray library does not work..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teqOFiQm_7mh",
        "outputId": "4667e43e-3b56-45be-b87e-c0e0e23df13c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.14.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (24.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.31.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.35.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2024.2.2)\n"
          ]
        }
      ]
    }
  ]
}